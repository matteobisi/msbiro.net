---
title: "The Silent Heist: How Distillation Attacks Are Reshaping the Global AI Landscape"
date: 2026-02-23
tags: ["ai", "cybersecurity", "anthropic", "distillation", "geopolitics"]
author: "Matteo Bisi"
showToc: true
TocOpen: false
draft: false
hidemeta: false
comments: false
description: "An analysis of the recent distillation attacks against Anthropic's Claude models by three major Chinese AI companies, explaining what distillation is and its implications for the global AI landscape."
canonicalURL: "https://www.msbiro.net/posts/distillation-attacks-anthropic-vs-chinese-ai/"
disableShare: true
hideSummary: false
searchHidden: false
ShowReadingTime: true
ShowBreadCrumbs: true
ShowPostNavLinks: true
ShowWordCount: true
ShowRssButtonInSectionTermList: true
UseHugoToc: true
cover:
    image: "https://www.msbiro.net/social-image.png"
    alt: "AI abstract network representing distillation"
    caption: "AI model distillation"
    relative: false
    hidden: true
editPost:
    URL: "https://github.com/matteobisi/msbiro.net/tree/main/content"
    Text: "Suggest Changes"
    appendFilePath: true
---

## Introduction to the AI Frontier

The AI race has largely boiled down to a high-stakes contest between the US and China. On one side, established US companies like Anthropic, OpenAI, Google, and X have continuously pushed the boundaries of frontier AI models. Anthropic, the research lab behind Claude, is best known for its focus on AI safety and its unique 'constitutional' approach to alignment.

Meanwhile, several Chinese tech firms have been fast-tracking models to compete with the best systems coming out of the US. This intense competition recently culminated in Anthropic revealing they had been the target of an industrial-scale "distillation attack" by three prominent Chinese AI laboratories.

## What is a Distillation Attack?

In the context of machine learning, "distillation" is generally a legitimate technique. It involves using the outputs of a very large, powerful model (the "teacher") to train a smaller, more efficient model (the "student"). This lets the smaller model act like its larger counterpart without needing nearly as much processing power.

However, a distillation *attack* occurs when this process is performed illicitly against a competitor's proprietary model. An attacking entity programmatically queries a target model—often millions of times—to generate high-quality training data. By doing so, they can acquire advanced AI capabilities at a fraction of the time and financial cost it took the original developer to train the frontier model from scratch. Anthropic noted that these attacks involved using commercial proxies to bypass regional restrictions and setting up "hydra clusters" of fraudulent accounts to scale their efforts undetected.

## The Three Attacks Unveiled by Anthropic

According to Anthropic's report, three major Chinese AI companies engaged in significant distillation campaigns to extract capabilities from the Claude models. The reported attacks are as follows:

### 1. DeepSeek
The first campaign was attributed to DeepSeek, involving over 150,000 illicit exchanges. Their operation targeted reasoning capabilities across diverse tasks, employing Claude as a reward model for their own reinforcement learning. DeepSeek generated synchronized traffic across multiple accounts, utilizing identical technical patterns and shared payment methods. They also prodded Claude to explain its logic step-by-step, effectively harvesting 'chain-of-thought' data and looking for ways to navigate sensitive topics.

### 2. Moonshot AI
Moonshot AI scaled things up significantly, logging over 3.4 million exchanges. Their efforts were focused on acquiring capabilities related to agentic reasoning, tool use, computer vision, and coding. To obscure their activities, Moonshot employed hundreds of fraudulent accounts spanning multiple access pathways. During the later phases of the campaign, they made targeted attempts to extract and reconstruct the underlying reasoning traces generated by Claude.

### 3. MiniMax
MiniMax ran the biggest operation of the three, racking up more than 13 million exchanges. Their attack targeted agentic coding, orchestration, and tool use capabilities. Anthropic actually caught this campaign while it was live, giving them a front-row seat to the attackers' methods. When Anthropic released a new model, MiniMax rapidly pivoted their strategy, redirecting nearly half of their query traffic within 24 hours to capture capabilities from the newly released system.

## Outro and Implications

While Europe continues to focus on regulatory frameworks like the AI Act and hopes to cultivate domestic AI champions capable of competing with State-of-the-Art models, the fundamental market reality remains a contest between the US and China. 

The Anthropic report adds a significant new layer to this geopolitical and technological rivalry. It'll be fascinating to see if these companies respond to such a public call-out. The tech community is familiar with controversies regarding the data sources used during initial model training (like web scraping copyrighted material), but deliberately performing a distillation attack to essentially mirror a competitor's multi-million-dollar AI training represents a distinctly different and aggressive approach.

The reaction from US regulators will also be a major factor to watch. Given Anthropic's concerns that these actions undermine US export controls and pose national security risks, there may be increasing pressure to create a unified, stringent front to secure frontier AI models against such extraction behaviors in the future.

### References
*   [Detecting and preventing distillation attacks | Anthropic](https://www.anthropic.com/news/detecting-and-preventing-distillation-attacks)
